{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz1hqb2oWg96"
      },
      "source": [
        "# Семинар 7. Инструменты для работы с языком"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXhKw6r2Wg97"
      },
      "source": [
        "... или зачем нужна предобработка.\n",
        "\n",
        "Раньше мы смотрели на светлую сторону анализа данных - построение моделей. Теперь попробуем глубже посмотреть на часть про предобработку данных. Задача предобработки особенно актуальна, если мы имеем дело с текстами."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvOWdHcUWg98"
      },
      "source": [
        "## Задача: классификация твитов по тональности\n",
        "\n",
        "У нас есть выборка из твитов.\n",
        "Нам известна эмоциональная окраска каждого твита из выборки: положительная или отрицательная. Задача состоит в построении модели, которая по тексту твита предсказывает его эмоциональную окраску.\n",
        "\n",
        "Классификацию по тональности используют в рекомендательных системах, чтобы понять, понравилось ли людям кафе, кино, etc.\n",
        "\n",
        "Скачиваем выборку ([источник](http://study.mokoron.com/)): [положительные](https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0), [отрицательные](https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GYvTkhGnWg98"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('negative.csv', <http.client.HTTPMessage at 0x7f2eacf2d1b0>)"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import urllib.request\n",
        "\n",
        "url1 = 'https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=1'\n",
        "url2 = 'https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=1'\n",
        "\n",
        "urllib.request.urlretrieve(url1, 'positive.csv')\n",
        "urllib.request.urlretrieve(url2, 'negative.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IX-AeH8RWg9-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # библиотека для удобной работы с датафреймами\n",
        "import numpy as np # библиотека для удобной работы со списками и матрицами\n",
        " \n",
        "# библиотека, где реализованы основные алгоритмы машинного обучения\n",
        "from sklearn.metrics import * \n",
        "from sklearn.model_selection import train_test_split \n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZtGDKFvQYEDO",
        "outputId": "7f32dd9f-1f90-4193-8b7c-3a594c0af205"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\"408906692374446080\";\"1386325927\";\"pleease_shut_up\";\"@first_timee хоть я и школота, но поверь, у нас то же самое :D общество профилирующий предмет типа)\";\"1\";\"0\";\"0\";\"0\";\"7569\";\"62\";\"61\";\"0\"\n",
            "\"408906692693221377\";\"1386325927\";\"alinakirpicheva\";\"Да, все-таки он немного похож на него. Но мой мальчик все равно лучше:D\";\"1\";\"0\";\"0\";\"0\";\"11825\";\"59\";\"31\";\"2\"\n",
            "\"408906695083954177\";\"1386325927\";\"EvgeshaRe\";\"RT @KatiaCheh: Ну ты идиотка) я испугалась за тебя!!!\";\"1\";\"0\";\"1\";\"0\";\"1273\";\"26\";\"27\";\"0\"\n",
            "\"408906695356973056\";\"1386325927\";\"ikonnikova_21\";\"RT @digger2912: \"\"Кто то в углу сидит и погибает от голода, а мы ещё 2 порции взяли, хотя уже и так жрать не хотим\"\" :DD http://t.co/GqG6iuE2…\";\"1\";\"0\";\"1\";\"0\";\"1549\";\"19\";\"17\";\"0\"\n",
            "\"408906761416867842\";\"1386325943\";\"JumpyAlex\";\"@irina_dyshkant Вот что значит страшилка :D\n",
            "Но блин,посмотрев все части,у тебя создастся ощущение,что авторы курили что-то :D\";\"1\";\"0\";\"0\";\"0\";\"597\";\"16\";\"23\";\"1\"\n",
            "\"408906761769598976\";\"1386325943\";\"JustinB94262583\";\"ну любишь или нет? — Я не знаю кто ты бля:D http://t.co/brf9eNg1U6\";\"1\";\"0\";\"0\";\"0\";\"40\";\"6\";\"16\";\"0\"\n",
            "\"408906762436481024\";\"1386325943\";\"twinkleAYO\";\"RT @SpoonLamer: Ох,900 :D ну это конечно же @twinkleAYO . Чтобы у нее было много друзей, ведь она такая мимими &lt;3\";\"1\";\"0\";\"1\";\"0\";\"5169\";\"58\";\"43\";\"2\"\n",
            "\"408906764114206720\";\"1386325944\";\"pycalyruhog\";\"RT @veregijytaqo: У тебя есть ухажёр? Нет - мои уши не кто не жрёт :D\";\"1\";\"0\";\"2\";\"0\";\"393\";\"112\";\"153\";\"0\"\n",
            "\"408906764608749568\";\"1386325944\";\"grishintv\";\"Поприветствуем моего нового читателя @Alexey1789 ;)\";\"1\";\"0\";\"0\";\"0\";\"5872\";\"1387\";\"1431\";\"12\"\n"
          ]
        }
      ],
      "source": [
        "!head positive.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVsSSqFKbAWL"
      },
      "source": [
        "Откроем файлы и создадим массив из текстов и правильных меток для твитов.\n",
        "Сначала идут положительные твиты, потом отрицательные."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "aBrSNCKVWg9_"
      },
      "outputs": [],
      "source": [
        "# загружаем положительные твиты\n",
        "positive = pd.read_csv('positive.csv', sep=';', usecols=[3], names=['text']) \n",
        "positive['label'] = ['positive'] * len(positive) # устанавливаем метки\n",
        " \n",
        "# загружаем отрицательные твиты\n",
        "negative = pd.read_csv('negative.csv', sep=';', usecols=[3], names=['text'])\n",
        "negative['label'] = ['negative'] * len(negative) # устанавливаем метки\n",
        " \n",
        "# соединяем вместе\n",
        "df = pd.concat([positive, negative])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "В этом коде происходит две вещи:\n",
        "\n",
        "Сначала с помощью функции pd.read_csv из библиотеки pandas загружается CSV-файл с именем 'positive.csv'. Параметр sep=';' указывает, что в этом файле в качестве разделителя используется символ ';'. Параметр usecols=[3] говорит о том, что нужно загрузить только одну колонку с индексом 3 (индексация начинается с 0). Параметр names=['text'] задает имя этой колонке - 'text'. Результат загрузки сохраняется в переменной positive, которая теперь является DataFrame'ом.\n",
        "\n",
        "Затем в этот DataFrame добавляется новая колонка с именем 'label'. Всем строкам этой колонки присваивается значение 'positive'. Это делается с помощью выражения ['positive'] * len(positive), которое создает список из строки 'positive', повторенной столько раз, сколько строк в DataFrame positive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfjWsULtaGtf"
      },
      "source": [
        "Посмотрим на полученные данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "aqudQEvUWg-D",
        "outputId": "38ce5973-7d39-4f43-c778-b9aad72bad9b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>15931</th>\n",
              "      <td>RT @Blawar_1337: Теперь у нас с @Wake_UA появи...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59532</th>\n",
              "      <td>с днём рождения зайка*))) ухх погуляем мы сего...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47185</th>\n",
              "      <td>RT @Shumkova0406199: @ann_safina Вов вов вов А...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42002</th>\n",
              "      <td>Надо выдернуть звуковую дорожку из \"Доктора Ка...</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109035</th>\n",
              "      <td>@_hassliebe_ может все таки на этой неделе вер...</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                     text     label\n",
              "15931   RT @Blawar_1337: Теперь у нас с @Wake_UA появи...  positive\n",
              "59532   с днём рождения зайка*))) ухх погуляем мы сего...  positive\n",
              "47185   RT @Shumkova0406199: @ann_safina Вов вов вов А...  negative\n",
              "42002   Надо выдернуть звуковую дорожку из \"Доктора Ка...  positive\n",
              "109035  @_hassliebe_ может все таки на этой неделе вер...  negative"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(5, random_state=40) # выводим 5 случайных примеров"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubV1-2JjaZuC"
      },
      "source": [
        "Разбиваем данные на обучающую и тестовую выборки с помощью функции ```train_test_split()``` из **sklearn**:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RscvM_TWg-F",
        "outputId": "64974685-3988-4f17-df11-351dc2b4f27d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "52772     хочу убивать людей, но меня останавливает УК Р...\n",
              "16875     Я уже хочу говорить фразу: \"За такие шутки, в ...\n",
              "31371     @EugeneKarnak та фигня это репейное ( даже на ...\n",
              "79679     @keinesommer @sladkobedrenna красивая упаковка...\n",
              "55126     Некоторым в группе сделала подарки на рождеств...\n",
              "                                ...                        \n",
              "72867     @ecoprettiness что ты делаешь в четверг в 16:1...\n",
              "24453     а вот в моём детстве нельзя было привлечь вним...\n",
              "105206    @madya_kabutova дададааа как будто мы не знаем...\n",
              "10098     теперь я могу разобрать горелку Kovea Mooonwak...\n",
              "12258     Вот она \"демократическая\" поправка в Избирател...\n",
              "Name: text, Length: 170125, dtype: object"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df.text, df.label)\n",
        "x_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zpq4QOU5Wg-H"
      },
      "source": [
        "## Baseline: классификация необработанных n-грамм\n",
        "\n",
        "* Сейчас мы попробуем получить преобразование предложений в численный вектор, с которым может работать стандартный алгоритм машинного обучения, такой как логистическая регрессия. \n",
        "* Для этого нам понадобится познакомиться с понятием n-gram - самых мелких элементов предложения, с которыми можно работать. \n",
        "* Подсчитав количество этих n-грам в предложениях, мы получим искомые численные представления."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_7DyyXRWg-K"
      },
      "source": [
        "## Что такое n-граммы:\n",
        "\n",
        "Самые мелкие структуры языка, с которыми мы работаем, называются **n-граммами**.\n",
        "У n-граммы есть параметр n - количество слов, которые попадают в такое представление текста.\n",
        "* Если n = 1 - то мы смотрим на то, сколько раз каждое слово встретилось в тексте. Получаем _униграммы_\n",
        "* Если n = 2 - то мы смотрим на то, сколько раз каждая пара подряд идущих слов, встретилась в тексте. Получаем _биграммы_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "quiUoyqNb3WA"
      },
      "source": [
        "Функция для работы с n-граммами реализована в библиотке **nltk** (Natural Language ToolKit), импортируем эту функцию: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "hcrWxBzzWg-K"
      },
      "outputs": [],
      "source": [
        "from nltk import ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ib-zYTvfQq5"
      },
      "source": [
        "Прежде чем получать n-граммы, нужно разделить предложение на отдельные слова.  Для этого используем метод ```split()```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2Ql8Em4Wg-N",
        "outputId": "280b0ba2-6e25-4fa9-e62a-9691d9760f19"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Если', 'б', 'мне', 'платили', 'каждый', 'раз']"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sentence = 'Если б мне платили каждый раз'.split()\n",
        "sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6V5P2Jcc4Oy"
      },
      "source": [
        "Чтобы получить n-грамму для такой последовательности, используем функцию ```ngrams()```. \n",
        "\n",
        "На вход передается два параметра:\n",
        "* лист с разделенным на отдельные слова предложением (у нас он хранится в переменной ```sent```);\n",
        "* параметр n, определяющий, какой тип n-грамм мы хотим получить.\n",
        "\n",
        "\n",
        "Чтобы полученный объект отобразить, делаем из него ```list```. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9oqpykUc5e9",
        "outputId": "0992f744-6d5e-468e-9fc4-e66ceaa359ec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если',), ('б',), ('мне',), ('платили',), ('каждый',), ('раз',)]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sentence, 1)) # униграммы (1-граммы) - слова в предложении по отдельности (по одному слову) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Функция ngrams из библиотеки NLTK используется для создания n-грамм из предложения. N-грамма - это последовательность из n слов.\n",
        "\n",
        "В данном случае ngrams(sentence, 2) создает биграммы, то есть пары последовательных слов из предложения.\n",
        "\n",
        "Предположим, у вас есть предложение \"Я люблю программировать на Python\". Биграммы этого предложения будут: (\"Я\", \"люблю\"), (\"люблю\", \"программировать\"), (\"программировать\", \"на\"), (\"на\", \"Python\").\n",
        "\n",
        "Функция list используется для преобразования результата в список, так как ngrams возвращает генератор.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZKRhRlxfoj4"
      },
      "source": [
        "Аналогично мы можем получить биграммы - для этого заменяем параметр **n** в функции **ngrams** с 1 на 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzl6t5dpWg-P",
        "outputId": "e8ef0dc5-9ef7-4cce-c579-c2ab5b7dfcea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б'),\n",
              " ('б', 'мне'),\n",
              " ('мне', 'платили'),\n",
              " ('платили', 'каждый'),\n",
              " ('каждый', 'раз')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sentence, 2)) # биграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCkkFzWLWg-R",
        "outputId": "3ea85fba-4560-493f-c09d-1d291ad78a53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне'),\n",
              " ('б', 'мне', 'платили'),\n",
              " ('мне', 'платили', 'каждый'),\n",
              " ('платили', 'каждый', 'раз')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sentence, 3)) # триграммы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GygS6_fJWg-S",
        "outputId": "3761bc3b-b530-4031-b0db-3147d231f3c5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Если', 'б', 'мне', 'платили', 'каждый'),\n",
              " ('б', 'мне', 'платили', 'каждый', 'раз')]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(ngrams(sentence, 5)) # ... пентаграммы?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JewKs4XU-so"
      },
      "source": [
        "### Векторизаторы\n",
        "\n",
        "Векторизатор преобразует слово или набор слов в числовой вектор, понятный алгоритму машинного обучения, который привык работать с числовыми табличными данными.\n",
        "\n",
        "Ниже - пример преобразования слов в двумерных вектор, каждому слову соответствует точка на плоскости.\n",
        "\n",
        "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
        "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5hiNv2eVAc-"
      },
      "source": [
        "На начальном этапе нам будет достаточно тех инструментов, которые уже есть в знакомой нам библиотеке **sklearn**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "cPplZnxeVEBR"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression # можно заменить на любимый классификатор\n",
        "from sklearn.feature_extraction.text import CountVectorizer # модель \"мешка слов\", см. далее"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBN16KYZWg-U"
      },
      "source": [
        "Самый простой способ извлечь признаки из текстовых данных -- векторизаторы: `CountVectorizer` и `TfidfVectorizer`\n",
        "\n",
        "Объект `CountVectorizer` делает следующую вещь:\n",
        "* строит для каждого документа (каждой пришедшей ему строки) вектор размерности `n`, где `n` -- количество слов или n-грам во всём корпусе\n",
        "* заполняет каждый i-тый элемент количеством вхождений слова в данный документ\n",
        "\n",
        "<a href=\"https://drive.google.com/uc?id=1ukv-FTj0jeVdcgVlOaNBocUfNuYGGVZg\n",
        "\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1jHmkrGZTMawM46Yzxh243Ur1y5pYKzrl\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"600\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oklbwY_vWg-X"
      },
      "source": [
        "На рисунке пример векторизации для униграмм, но можно использовать любые n-граммы. Для этого у объекта ```CountVectorizer()``` есть параметр **ngram_range**, который отвечает за то, какие n-граммы мы используем в качестве признаов:<br/>\n",
        "ngram_range=(1, 1) -- униграммы<br/>\n",
        "ngram_range=(3, 3) -- триграммы<br/>\n",
        "ngram_range=(1, 3) -- униграммы, биграммы и триграммы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BjJmHbOvVmln"
      },
      "source": [
        "<a href=\"https://drive.google.com/uc?id=1ODNVK0fdLTX4nv6ob55ciUe37d1pio-D\" target=\"_blank\"><img src=\"https://drive.google.com/uc?id=1ODNVK0fdLTX4nv6ob55ciUe37d1pio-D\" \n",
        "alt=\"IMAGE ALT TEXT HERE\" width=\"800\" border=\"0\" /></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZLTEzNWxrx5X"
      },
      "source": [
        "Инициализируем ```CountVectorizer()```, указав в качестве признаков униграммы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hWKQcLfBWg-W"
      },
      "outputs": [],
      "source": [
        "vectorizer = CountVectorizer(ngram_range=(1, 1)) # инициализируем \"мешок слов\" для униграмм "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Команда vectorizer = CountVectorizer(ngram_range=(1, 1)) создает объект CountVectorizer с параметром ngram_range=(1, 1).\n",
        "\n",
        "CountVectorizer - это класс из библиотеки sklearn, который преобразует текстовые данные в векторы признаков. Каждому уникальному слову в тексте сопоставляется целочисленный идентификатор, а значение признака для данного слова в тексте равно количеству вхождений этого слова в текст.\n",
        "\n",
        "Параметр ngram_range=(1, 1) указывает, что векторизатор должен учитывать только униграммы, то есть отдельные слова. Если бы вы хотели учитывать, например, биграммы (пары слов), вы бы использовали ngram_range=(1, 2).\n",
        "\n",
        "Таким образом, эта команда создает объект CountVectorizer, который можно затем использовать для преобразования текстовых данных в векторы признаков, учитывая только униграммы.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-gFUNm6huAz"
      },
      "source": [
        "После инициализации _vectorizer_ можно обучить на наших данных. \n",
        "\n",
        "Для обучения используем обучающую выборку ```x_train```, но в отличие от классификатора мы используем метод ```fit_transform()```: сначала обучаем наш векторизатор, а потом сразу применяем его к нашему набору данных. Это похоже на то, как мы работали с label encoderом и one-hot-encoderом.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "TkJESwGfhq4v"
      },
      "outputs": [],
      "source": [
        "vectorized_x_train = vectorizer.fit_transform(x_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kbIKYfgyCHT-",
        "outputId": "904834fe-4673-49f2-cf59-f44b40935d2d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<170125x243804 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 1847487 stored elements in Compressed Sparse Row format>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorized_x_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPJDPt7xvWKe"
      },
      "source": [
        "Так как результат не зависит от порядка слов в текстах, то говорят, что такая модель представления текстов в виде векторов получается из *гипотезы представления текста как мешка слов*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\"Гипотеза представления текста как мешка слов\" (bag of words) - это подход к представлению текстовых данных для машинного обучения. В этом подходе текст представляется как мешок (или набор) слов, без учета грамматики и порядка слов, но с учетом частоты слов.\n",
        "\n",
        "В контексте векторизации текста, как в вашем примере с CountVectorizer, каждое уникальное слово в тексте представляется в виде признака в векторе, а значение этого признака - это количество раз, которое слово встречается в тексте.\n",
        "\n",
        "Таким образом, порядок слов в исходном тексте теряется, и текст превращается в набор слов (или \"мешок слов\"). Это может быть полезно для многих задач машинного обучения, где важна частота слов, а не их порядок."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyT1Kz6Ppt8n"
      },
      "source": [
        "В vectorizer.vocabulary_ лежит словарь, отображение слов в их индексы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72knqTvCWg-Y",
        "outputId": "5bf0e7b3-9f14-4c16-94cb-9791f7472fca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('rt', 74597),\n",
              " ('mr_malcev', 59533),\n",
              " ('пипец', 182397),\n",
              " ('завтро', 133145),\n",
              " ('первой', 179930),\n",
              " ('смены', 211532),\n",
              " ('daniilpanchenko', 23085),\n",
              " ('везет', 110835),\n",
              " ('нам', 163299),\n",
              " ('еще', 130686)]"
            ]
          },
          "execution_count": 19,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "list(vectorizer.vocabulary_.items())[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdKqMyRKjA-p"
      },
      "source": [
        "В нашей выборке 170125 текстов (твитов), в них встречается 243760 разных слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IjOD7nrsi2fc",
        "outputId": "8412e265-521e-47be-8a16-af5be3e16bae"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(170125, 243617)"
            ]
          },
          "execution_count": 20,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vectorized_x_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7geX_YHDjG9v"
      },
      "source": [
        "Так как теперь у нас есть **численное представление** и набор входных признаков, то мы можем обучить модель логистической регрессии (или любую другую из тех, на которые мы смотрели раньше, например, случайный лес)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCdROyxsWg-b",
        "outputId": "40eae481-3574-49b3-808e-27b281930770"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000, random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "LogisticRegression(max_iter=1000, random_state=42)"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "clf = LogisticRegression(random_state=42, max_iter=1000) # фиксируем random_state для воспроизводимости результатов\n",
        "clf.fit(vectorized_x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z9iIlzOjVUF"
      },
      "source": [
        "С тестовыми данными нужно проделать то же самое, что и с данными для обучения: сделать из текстов вектора, которые можно передавать в классификатор для прогноза класса объекта. \n",
        "\n",
        "У нас уже есть обученный векторизатор ```vectorizer```, поэтому используем метод ```transform()``` (просто применить его), а не ```fit_transform``` (обучить и применить)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "u2_AW8FpjWHQ"
      },
      "outputs": [],
      "source": [
        "vectorized_x_test = vectorizer.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahCRPAeWjtcl"
      },
      "source": [
        "Как раньше, для получения прогноза у обученного классификатора используем метод ```predict()```.\n",
        "\n",
        "С помощью функции ```classification_report()```, которая считает сразу несколько метрик качества классификации, посмотрим на то, насколько хорошо мы предсказываем положительную или отрицательную тональность твита ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "juvxbzinWg-d",
        "outputId": "9e10c88a-9bf4-474d-f45c-8da0a91238c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.76      0.76     28019\n",
            "    positive       0.77      0.77      0.77     28690\n",
            "\n",
            "    accuracy                           0.77     56709\n",
            "   macro avg       0.77      0.77      0.77     56709\n",
            "weighted avg       0.77      0.77      0.77     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred = clf.predict(vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "label\n",
            "positive    28690\n",
            "negative    28019\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(y_test.value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yiLk1P_xYQ2"
      },
      "source": [
        "## N-граммы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjy5ZPmwWg-j"
      },
      "source": [
        "Попробуем сделать то же самое, используя в качестве признаков триграммы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmQHqUpRWg-k",
        "outputId": "861c6ce8-6d05-44c0-b3ae-eee36438cba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.72      0.47      0.57     28019\n",
            "    positive       0.61      0.82      0.70     28690\n",
            "\n",
            "    accuracy                           0.65     56709\n",
            "   macro avg       0.67      0.64      0.63     56709\n",
            "weighted avg       0.66      0.65      0.63     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# инициализируем векторайзер \n",
        "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "trigram_vectorized_x_train = trigram_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "clf.fit(trigram_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторизатор к тестовым данным\n",
        "trigram_vectorized_x_test = trigram_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(trigram_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MonLW7AyWg-m"
      },
      "source": [
        "Как вы думаете, почему в результатах теперь такой разброс по сравнению с униграммами?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Качество модели машинного обучения при использовании триграмм может быть хуже по сравнению с униграммами по нескольким причинам:\n",
        "\n",
        "Разреженность данных: Триграммы создают гораздо больше уникальных признаков, многие из которых могут встречаться очень редко. Это может привести к \"разреженности\" данных, когда большинство признаков для каждого образца равны нулю. Модели могут быть менее эффективными при работе с такими разреженными данными.\n",
        "\n",
        "Переобучение: Из-за большого количества признаков модель может \"переобучиться\" на тренировочных данных, то есть слишком хорошо подстроиться под них, учтя даже случайные шумы. В результате на тестовых данных качество может быть хуже.\n",
        "\n",
        "Потеря важной информации: Триграммы учитывают контекст двух соседних слов, но при этом могут упустить более широкий контекст или важные отдельные слова. Униграммы, с другой стороны, не учитывают контекст, но могут лучше улавливать важность отдельных слов.\n",
        "\n",
        "Все эти факторы могут влиять на качество модели. Оптимальный выбор между униграммами, биграммами, триграммами и т.д. зависит от конкретной задачи и данных.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlWxW3e9Wg-m"
      },
      "source": [
        "## TF-IDF векторизация"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hCxZRtWg-m"
      },
      "source": [
        "`TfidfVectorizer` делает то же, что и `CountVectorizer`, но в качестве значений выдает **tf-idf** каждого слова.\n",
        "\n",
        "Как считается tf-idf:\n",
        "\n",
        "**TF (term frequency)** – относительная частотность слова в документе:\n",
        "$$ TF(t,d) = \\frac{n_{t}}{\\sum_k n_{k}} $$\n",
        "\n",
        "**IDF (inverse document frequency)** – обратная частота документов, в которых есть это слово:\n",
        "$$ IDF(t, D) = \\log \\frac{|D|}{|{d : t \\in d}|} $$\n",
        "\n",
        "Перемножаем их:\n",
        "$$TFIDF(t, d, D) = TF(t,d) \\times IDF(i, D)$$\n",
        "\n",
        "Сакральный смысл: если слово часто встречается в одном документе, но в целом по корпусу встречается в небольшом \n",
        "количестве документов, у него высокий TF-IDF."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Логический смысл TF-IDF следующий:\n",
        "\n",
        "TF (Term Frequency): Если слово встречается часто в документе, то, вероятно, оно важно для понимания содержания этого документа. Поэтому мы увеличиваем вес этого слова пропорционально его частоте.\n",
        "\n",
        "IDF (Inverse Document Frequency): Если слово встречается во многих документах, то, вероятно, оно менее дискриминативно или информативно для отдельного документа (например, общие слова типа \"и\", \"в\", \"на\"). Поэтому мы уменьшаем вес этого слова пропорционально числу документов, в которых оно встречается.\n",
        "\n",
        "Таким образом, TF-IDF дает высокий вес словам, которые встречаются часто в данном документе, но редко в других документах, что делает их потенциально важными для понимания содержания данного документа."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Fv7DfTkJWg-n"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "02f_zZm14PHM"
      },
      "source": [
        "Действуем аналогично, как с ```CountVectorizer()```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rMYiobAWg-p",
        "outputId": "43edfdae-88c6-4670-f50a-6ec6b5f47f95"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.73      0.77      0.75     28019\n",
            "    positive       0.77      0.72      0.74     28690\n",
            "\n",
            "    accuracy                           0.75     56709\n",
            "   macro avg       0.75      0.75      0.75     56709\n",
            "weighted avg       0.75      0.75      0.75     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# инициализируем векторизатор, в качестве переменных используем униграммы\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 5))\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "tfidf_vectorized_x_train = tfidf_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42, max_iter=1000)\n",
        "clf.fit(tfidf_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторизатор к тестовым данным\n",
        "tfidf_vectorized_x_test = tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(tfidf_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qkuods5LWg-q"
      },
      "source": [
        "В этот раз получилось хуже :( Вернёмся к `CountVectorizer()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D39SSh0zWg-r"
      },
      "source": [
        "## Токенизация\n",
        "\n",
        "Токенизировать - значит, поделить текст на части: слова, ключевые слова, фразы, символы и т.д., иными словами **токены**.\n",
        "\n",
        "Самый наивный способ токенизировать текст - разделить с помощью функции `split()`. Но `split` упускает очень много всего, например, не отделяет пунктуацию от слов. Кроме этого, есть ещё много менее тривиальных проблем, поэтому лучше использовать готовые токенизаторы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "hoSe08N2Wg-r"
      },
      "outputs": [],
      "source": [
        "import nltk # уже знакомая нам библиотека nltk\n",
        "from nltk.tokenize import word_tokenize # готовый токенизатор библиотеки nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiDt3L8Y8god"
      },
      "source": [
        "Чтобы использовать токенизатор ```word_tokenize```, нужно сначала скачать данные для nltk о пунктуации и стоп-словах. Это просто требование nltk, поэтому, особо не задумываясь, запустите следующую ячейку:  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPH3yMcumsdd",
        "outputId": "3a17fe98-5086-4d2d-d44d-b74bfa861573"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /home/codespace/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /home/codespace/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NfDb8D_9DqD"
      },
      "source": [
        "Применим токенизацию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrJDGpgYWg-4",
        "outputId": "16e3ccd4-8748-4f59-c12e-1989dd9feb1a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять', ':', '(']"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example = 'Но не каждый хочет что-то исправлять:('\n",
        "word_tokenize(example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxy7KGZI9bhK"
      },
      "source": [
        "Если использовать просто ```split()```, то грустный смайлик :( не отделяется от слова \"исправлять\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p52dIuSI9W6o",
        "outputId": "ab8a0145-202d-4cbd-dd31-e5416691d9d9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Но', 'не', 'каждый', 'хочет', 'что-то', 'исправлять:(']"
            ]
          },
          "execution_count": 41,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "example.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_702Dg5OWg-5"
      },
      "source": [
        "В nltk вообще есть довольно много токенизаторов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ps8oPYoTWg-6",
        "outputId": "e1931187-69c4-403b-b114-5c798119aa74"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['BlanklineTokenizer',\n",
              " 'LegalitySyllableTokenizer',\n",
              " 'LineTokenizer',\n",
              " 'MWETokenizer',\n",
              " 'NLTKWordTokenizer',\n",
              " 'PunktSentenceTokenizer',\n",
              " 'RegexpTokenizer',\n",
              " 'ReppTokenizer',\n",
              " 'SExprTokenizer',\n",
              " 'SpaceTokenizer',\n",
              " 'StanfordSegmenter',\n",
              " 'SyllableTokenizer',\n",
              " 'TabTokenizer',\n",
              " 'TextTilingTokenizer',\n",
              " 'ToktokTokenizer',\n",
              " 'TreebankWordDetokenizer']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from nltk import tokenize\n",
        "dir(tokenize)[:16]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lmnGCL5iWg-8"
      },
      "source": [
        "Они умеют выдавать индексы в строке для начала и конца каждого слова-токена:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jejj5X7QWg-8",
        "outputId": "473ce35d-c70d-4949-b27b-0876e42e0c8f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(0, 2), (3, 5), (6, 12), (13, 18), (19, 25), (26, 38)]"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wh_tok = tokenize.WhitespaceTokenizer()\n",
        "list(wh_tok.span_tokenize(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-wf6A1EWg--"
      },
      "source": [
        "Некторые токенизаторы ведут себя специфично:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2REwpHGWWg-_",
        "outputId": "f3b5ec5d-056f-4eb9-f9f8-f743abc59a89"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['do', \"n't\", 'stop', 'me']"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize.TreebankWordTokenizer().tokenize(\"don't stop me\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tckre90JWg_B"
      },
      "source": [
        "А некоторые -- вообще не для текста на естественном языке:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1Ml3xtaWg_D",
        "outputId": "8be1c971-a9c1-428b-b012-f39031bd8a8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['(a (b c))', 'd', 'e', '(f)']"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tokenize.SExprTokenizer().tokenize(\"(a (b c)) d e (f)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bM2kvAo0_b93"
      },
      "source": [
        "**Правильный токенизатор подбирается исходя из требований задачи!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhVrgkSaWg_K"
      },
      "source": [
        "## Стоп-слова и пунктуация\n",
        "\n",
        "**Стоп-слова** - это слова, которые часто встречаются практически в любом тексте и ничего интересного не говорят о конретном документе. Для модели это просто шум. А шум нужно убирать. По аналогичной причине убирают и пунктуацию."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld-h6WKyWg_K",
        "outputId": "08cb7037-3af0-436b-e74e-f8ea811200c2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
          ]
        }
      ],
      "source": [
        "# импортируем стоп-слова из библиотеки nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# посмотрим на стоп-слова для русского языка\n",
        "print(stopwords.words('russian'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ihn2Y_SzBU57"
      },
      "source": [
        "*Знаки* пунктуации лучше импортировать из модуля **String**. В нем хранятся различные наборы констант для работы со строками (пунктуация, алфавит и др.). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "x64U3FdPWg_M",
        "outputId": "fe7dbcb8-46d8-491f-9006-9c7ff93d7240"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from string import punctuation\n",
        "punctuation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9LGTLaEBwsC"
      },
      "source": [
        "Объединим стоп-слова и знаки пунктуации вместе и запишем в переменную ```noise```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "VfH5y50wWg_O"
      },
      "outputs": [],
      "source": [
        "noise = stopwords.words('russian') + list(punctuation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3gweXaWWg_P"
      },
      "source": [
        "Теперь нужно обучать нашу модель с учетом новых знаний про токенизацию и стоп-слова. \n",
        "\n",
        "Для этого мы можем собрать новый векторизатор, передав ему на вход:\n",
        "* какие n-граммы нам нужны, параметр **ngram_range**;\n",
        "* какой токенизатор мы используем, параметр **tokenizer**;\n",
        "* какие у нас стоп-слова, параметр **stop_words**.\n",
        "\n",
        "*Напоминание:* мы используем готовый токенизатор ```word_tokenize```, а стоп-слова хранятся в переменной ```noise```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "fbXrVeRRuAxx"
      },
      "outputs": [],
      "source": [
        "# инициализируем умный векторайзер \n",
        "smart_vectorizer = CountVectorizer(ngram_range=(1, 1), \n",
        "                                   tokenizer=word_tokenize, \n",
        "                                   stop_words=noise)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Nc6D-nwWg_P",
        "outputId": "8c46a805-ec61-4049-f1e4-1d66aceba6b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n",
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['``'] not in stop_words.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       0.76      0.80      0.78     28019\n",
            "    positive       0.79      0.76      0.78     28690\n",
            "\n",
            "    accuracy                           0.78     56709\n",
            "   macro avg       0.78      0.78      0.78     56709\n",
            "weighted avg       0.78      0.78      0.78     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# обучаем его и сразу применяем к x_train\n",
        "smart_vectorized_x_train = smart_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(smart_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "smart_vectorized_x_test = smart_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(smart_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYWB1foQWg_T"
      },
      "source": [
        "Получилось лучше: accuracy выше, а также заметно подрос recall у негативного класса. \n",
        "\n",
        "Что ещё можно сделать?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsRf9T_SWg_U"
      },
      "source": [
        "## Бонус*: Лемматизация\n",
        "\n",
        "**Лемматизация** – это сведение разных форм одного слова к начальной форме – **лемме**. Почему это хорошо?\n",
        "* Во-первых, естественно рассматривать как отдельный признак каждое *слово*, а не каждую его отдельную форму.\n",
        "* Во-вторых, некоторые стоп-слова стоят только в начальной форме, и без лематизации выкидываем мы только её.\n",
        "\n",
        "Для русского есть хороших лемматизатор pymorphy. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylKZG2MwWg_f"
      },
      "source": [
        "### [Pymorphy](http://pymorphy2.readthedocs.io/en/latest/)\n",
        "Это модуль на питоне, довольно быстрый и с кучей функций."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcYWYq4BzOon",
        "outputId": "f0a55609-37f9-4268-9b5e-4fb47cc96fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymorphy2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/57/b2ff2fae3376d4f3c697b9886b64a54b476e1a332c67eee9f88e7f1ae8c9/pymorphy2-0.9.1-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.2MB/s \n",
            "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/79/bea0021eeb7eeefde22ef9e96badf174068a2dd20264b9a378f2be1cdd9e/pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2MB)\n",
            "\u001b[K     |████████████████████████████████| 8.2MB 7.4MB/s \n",
            "\u001b[?25hCollecting dawg-python>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6a/84/ff1ce2071d4c650ec85745766c0047ccc3b5036f1d03559fd46bb38b5eeb/DAWG_Python-0.7.2-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.6/dist-packages (from pymorphy2) (0.6.2)\n",
            "Installing collected packages: pymorphy2-dicts-ru, dawg-python, pymorphy2\n",
            "Successfully installed dawg-python-0.7.2 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844\n"
          ]
        }
      ],
      "source": [
        "# устанавливаем pymorphy2\n",
        "!pip install pymorphy2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqdT2pmRFn2F"
      },
      "source": [
        "В pymorphy2 для морфологического анализа слов есть ```MorphAnalyzer()```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "m4nRuUu2Wg_g"
      },
      "outputs": [],
      "source": [
        "from pymorphy2 import MorphAnalyzer\n",
        "pymorphy2_analyzer = MorphAnalyzer()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Egd6KdqzWg_h"
      },
      "source": [
        "pymorphy2 работает с отдельными словами. Если дать ему на вход предложение - он его просто не лемматизирует, т.к. не понимает:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6hdm1KBFx18",
        "outputId": "1c1a42ec-bf42-4fb1-f2d3-f43e96bf9bb4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Если', 'б', 'мне', 'платили', 'каждый', 'раз']"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sent = ['Если', 'б', 'мне', 'платили', 'каждый', 'раз']\n",
        "sent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cr1C2beNF3vE"
      },
      "source": [
        "Лемматизируем слово \"платили\" из предложения ```sent``` с помощью метода ```parse()```:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q3zNlPBWg_i",
        "outputId": "dda2cc39-d942-4c11-9a5b-6737242a6390"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[Parse(word='платили', tag=OpencorporaTag('VERB,impf,tran plur,past,indc'), normal_form='платить', score=1.0, methods_stack=((DictionaryAnalyzer(), 'платили', 2472, 10),))]"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ana = pymorphy2_analyzer.parse(sent[3])\n",
        "ana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2O2BL4_GJzq"
      },
      "source": [
        "Выведем его нормальную форму:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7-zp0KZLWg_p",
        "outputId": "5deda388-0557-4d68-ff17-9fcc8e745f95"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'платить'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ana[0].normal_form"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hedBdcYWhAH"
      },
      "source": [
        "## О важности эксплоративного анализа\n",
        "\n",
        "Но иногда пунктуация бывает и не шумом - главное отталкиваться от задачи. Что будет если вообще не убирать пунктуацию?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhZMwsY5WhAI",
        "outputId": "7e3b76a5-ebd3-42e7-89bb-9f0d8fc919b1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      1.00      1.00     28019\n",
            "    positive       1.00      1.00      1.00     28690\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# инициализируем умный векторайзер stop-words НЕ ИСПОЛЬЗУЕМ!\n",
        "alternative_tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), \n",
        "                                               tokenizer=word_tokenize)\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "alternative_tfidf_vectorized_x_train = alternative_tfidf_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(alternative_tfidf_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "alternative_tfidf_vectorized_x_test = alternative_tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(alternative_tfidf_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSFebp_1WhAK"
      },
      "source": [
        "Шок! Стоило оставить пунктуацию -- и все метрики равны 1. Как это получилось? Среди неё были очень значимые токены (как вы думаете, какие?)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FEQbCtidWhAP"
      },
      "source": [
        "Посмотрим, как один из супер-значительных токенов справится с классификацией безо всякого машинного обучения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhUG9qWuWhAQ",
        "outputId": "822c7276-1197-4dfa-ffa8-cb90e127d159"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.85      0.92     32943\n",
            "    positive       0.83      1.00      0.91     23766\n",
            "\n",
            "    accuracy                           0.91     56709\n",
            "   macro avg       0.91      0.93      0.91     56709\n",
            "weighted avg       0.93      0.91      0.91     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "cool_token = ')'\n",
        "pred = ['positive' if cool_token in tweet else 'negative' for tweet in x_test]\n",
        "print(classification_report(pred, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrqW55jgWhAR"
      },
      "source": [
        "## Символьные n-граммы\n",
        "\n",
        "Теперь в качестве фичей используем, например, униграммы символов. Для этого необходимо установить в ```CountVectorizer()``` параметр ```analyzer = 'char'```, то есть анализировать символы."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4lNhEmyWhAU",
        "outputId": "ab9263dc-0183-4b30-e093-745b93441427"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "    negative       1.00      0.99      1.00     28023\n",
            "    positive       0.99      1.00      1.00     28686\n",
            "\n",
            "    accuracy                           1.00     56709\n",
            "   macro avg       1.00      1.00      1.00     56709\n",
            "weighted avg       1.00      1.00      1.00     56709\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# инициализируем векторайзер для символов\n",
        "char_vectorizer = CountVectorizer(analyzer='char', ngram_range=(1, 1))\n",
        "\n",
        "# обучаем его и сразу применяем к x_train\n",
        "char_vectorized_x_train = char_vectorizer.fit_transform(x_train)\n",
        "\n",
        "# инициализируем и обучаем классификатор\n",
        "clf = LogisticRegression(random_state=42)\n",
        "clf.fit(char_vectorized_x_train, y_train)\n",
        "\n",
        "# применяем обученный векторайзер к тестовым данным\n",
        "char_vectorized_x_test = char_vectorizer.transform(x_test)\n",
        "\n",
        "# получаем предсказания и выводим информацию о качестве\n",
        "pred = clf.predict(char_vectorized_x_test)\n",
        "print(classification_report(y_test, pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLMMicsFWhAY"
      },
      "source": [
        "Из предыдущего раздела уже понятно, почему на этих данных точность равна 1. \n",
        "\n",
        "Символьные n-граммы используются, например, для задачи определения языка. Ещё одна замечательная особенность признаков-символов - для них не нужна токенизация и лемматизация, можно использовать такой подход для языков, у которых нет готовых анализаторов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QYTwyMtWhAZ"
      },
      "source": [
        "## Бонус***: регулярные выражения\n",
        "\n",
        "Регулярные выражения - способ поиска и анализа строк. Например, можно понять, какие даты в наборе строк представлены в формате DD/MM/YYYY, а какие - в других форматах. \n",
        "\n",
        "Или бывает, например, что перед работой с текстом, надо почистить его от своеобразного мусора: упоминаний пользователей, url и так далее.\n",
        "\n",
        "Навык полезный, давайте в нём тоже потренируемся.\n",
        "\n",
        "Для работы с регулярными выражениями есть библиотека **re**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "VaUW5S4gWhAb"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D6aYh7Osl8xr"
      },
      "source": [
        "В регулярных выражениях, кроме привычных символов-букв, есть специальные символы:\n",
        "* **?а** - ноль или один символ **а**\n",
        "* **+а** - один или более символов **а**\n",
        "* **\\*а** - ноль или более символов **а** (не путать с +)\n",
        "* **.** - любое количество любого символа\n",
        "\n",
        "Пример:\n",
        "Выражению \\*a?b. соответствуют последовательности a, ab, abc, aa, aac НО НЕ abb!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q7zOFFA3l_KQ"
      },
      "source": [
        "Рассмотрим подробно несколько наиболее полезных функций:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbJrUpARWhAd"
      },
      "source": [
        "### findall\n",
        "возвращает список всех найденных непересекающихся совпадений.\n",
        "\n",
        "Регулярное выражение **ab+c.**: \n",
        "* **a** - просто символ **a**\n",
        "* **b+** - один или более символов **b**\n",
        "* **c** - просто символ **c**\n",
        "* **.** - любой символ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2athHzKuWhAd",
        "outputId": "909c82c2-3590-4cd7-de11-61b1206d54bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['abcd', 'abca']\n"
          ]
        }
      ],
      "source": [
        "result = re.findall('ab+c.', 'abcdefghijkabcabcxabc') \n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9FpIw5RWhAf"
      },
      "source": [
        "Вопрос на внимательность: почему нет abcx?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5ttzoxEWhAg"
      },
      "source": [
        "**Задание**: вернуть список первых двух букв каждого слова в строке, состоящей из нескольких слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZR2AEq3WhAg",
        "outputId": "af2d7380-9f5f-4d51-821d-0e434da26b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['he', 'll']\n",
            "['wo', 'rl']\n",
            "[]\n",
            "['am']\n",
            "['Al', 'in']\n"
          ]
        }
      ],
      "source": [
        "sent = 'hello world, I am Alina'.split()\n",
        "for s in sent:\n",
        "  print(re.findall('\\w{2}',s))\n",
        "#result=re.findall('\\w{2}','hello world')\n",
        "#result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MI18l-l9WhAk"
      },
      "source": [
        "### split\n",
        "разделяет строку по заданному шаблону\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVKdRoc1WhAl",
        "outputId": "d79c7c00-f57c-43f1-e546-0b72bb0be900"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['its', ', bits', ', teenie, weenie']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie')  # разбивает строку по разделителю ,\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10u5efuSWhAm"
      },
      "source": [
        "можно указать максимальное количество разбиений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9U9EQZMwWhAn",
        "outputId": "6cea2b27-2e21-4c93-ac19-c94f930df793"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['itsy', ' bitsy', ' teenie, weenie']\n"
          ]
        }
      ],
      "source": [
        "result = re.split(',', 'itsy, bitsy, teenie, weenie', maxsplit=2) \n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0EMcMyflWhAp"
      },
      "source": [
        "**Задание**: разбейте строку, состоящую из нескольких предложений, по точкам, но не более чем на 3 предложения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "dVgPSjEOWhAp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Это первое предложение', ' Это второе предложение', ' Это третье предложение']\n"
          ]
        }
      ],
      "source": [
        "text = \"Это первое предложение. Это второе предложение. Это третье предложение. Это четвертое предложение. Это пятое предложение.\"\n",
        "sentences = re.split('\\.', text)[:3] # разбиваем текст на предложения по точке и берем первые 3 предложения \n",
        "print(sentences)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wrEGqBSWhAr"
      },
      "source": [
        "### sub\n",
        "ищет шаблон в строке и заменяет все совпадения на указанную подстроку\n",
        "\n",
        "параметры: (pattern, repl, string)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az3KxKWwWhAr",
        "outputId": "4f7e6c80-1dbb-42de-9124-61fa5d20d4ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bbcbbc\n"
          ]
        }
      ],
      "source": [
        "result = re.sub('a', 'b', 'abcabc') # ищем все вхождения 'a' и заменяем их на 'b'\n",
        "print (result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD0n7_HPWhAt"
      },
      "source": [
        "**Задание**: напишите регулярное выражение, которое позволит заменить все цифры в строке на \"DIG\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "s_Sdu7xlWhAu"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DIG DIG DIG DIG f s s cDIGDIGDIGDIG DIG\n"
          ]
        }
      ],
      "source": [
        "result = re.sub('\\d', 'DIG', '1 2 3 5 f s s c5443 6')\n",
        "print (result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8__oi1PWhAv"
      },
      "source": [
        "**Задание**: напишите  регулярное выражение, которое позволит убрать url из строки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KwNS9zt4WhAv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gStgBJy2WhAx"
      },
      "source": [
        "### compile\n",
        "компилирует регулярное выражение в отдельный объект"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "JstTupisWhAy",
        "outputId": "1e270397-7378-4b47-8346-126fcfdde823"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Слова', 'Да', 'больше', 'ещё', 'больше', 'слов', 'Что-то', 'ещё']"
            ]
          },
          "execution_count": 4,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Пример: построение списка всех слов строки:\n",
        "prog = re.compile('[А-Яа-яё\\-]+')\n",
        "prog.findall(\"Слова? Да, больше, ещё больше слов! Что-то ещё.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXEXc3G0WhA2"
      },
      "source": [
        "**Задание**: для выбранной строки постройте список слов, которые длиннее трех символов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFvnIWbUWhA2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQDNZ3HQWhA3"
      },
      "source": [
        "**Задание**: вернуть список доменов (@gmail.com) из списка адресов электронной почты:\n",
        "\n",
        "```\n",
        "abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "haZ5qn3DWhA3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['@gmail.com', '@test.in', '@analyticsvidhya.com', '@rest.biz']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "emails = \"abc.test@gmail.com, xyz@test.in, test.first@analyticsvidhya.com, first.test@rest.biz\"\n",
        "\n",
        "domains = re.findall(r'@\\w+\\.\\w+', emails)\n",
        "\n",
        "print(domains)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtlJMt0YWhA6"
      },
      "source": [
        "Если всё ещё осталось время: [регулярочный кроссворд ¯\\_(ツ)_/¯](https://mariolurig.com/crossword/)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "gStgBJy2WhAx"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
